{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T15:57:02.350027Z",
     "iopub.status.busy": "2025-08-07T15:57:02.349768Z",
     "iopub.status.idle": "2025-08-07T15:57:06.427148Z",
     "shell.execute_reply": "2025-08-07T15:57:06.426515Z",
     "shell.execute_reply.started": "2025-08-07T15:57:02.350003Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 15:57:02.749580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754582222.782243  182357 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754582222.795108  182357 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time \n",
    "import pickle\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T15:57:06.462837Z",
     "iopub.status.busy": "2025-08-07T15:57:06.462563Z",
     "iopub.status.idle": "2025-08-07T15:57:06.490200Z",
     "shell.execute_reply": "2025-08-07T15:57:06.489466Z",
     "shell.execute_reply.started": "2025-08-07T15:57:06.462805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, action_space_size, learning_rate=0.001):\n",
    "        self.state_shape = (23, 23, 4)\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_layer = Input(shape=self.state_shape, name='matrix_input')\n",
    "        x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(input_layer)\n",
    "        x = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "\n",
    "        policy_output = Dense(self.action_space_size, activation='softmax', name='policy_output')(x)\n",
    "\n",
    "        value_output = Dense(1, activation='tanh', name='value_output')(x)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=[policy_output, value_output])\n",
    "        model.compile(optimizer=Adam(self.learning_rate),\n",
    "                      loss={'policy_output': 'categorical_crossentropy', 'value_output': 'mean_squared_error'})\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def predict(self, matrix_state):\n",
    "        return self.model.predict(np.expand_dims(matrix_state, axis=0), verbose=0)\n",
    "\n",
    "    def train(self, states, target_policies, target_values, batch_size=32):\n",
    "        return self.model.fit(states, [target_policies, target_values], batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Alpha Zero Agent</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T15:57:06.492632Z",
     "iopub.status.busy": "2025-08-07T15:57:06.492433Z",
     "iopub.status.idle": "2025-08-07T15:57:06.516816Z",
     "shell.execute_reply": "2025-08-07T15:57:06.516258Z",
     "shell.execute_reply.started": "2025-08-07T15:57:06.492616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AlphaZeroAgent:\n",
    "    def _zero_array_factory(self):\n",
    "        '''Inorder to save mcts details pickles needs a named function to look up, \n",
    "            using lambdas results in unpicklable objects'''\n",
    "        return np.zeros(self.env.action_space.n)\n",
    "\n",
    "    def __init__(self, env, network, simulations_per_move=50, max_depth=25, c_puct=1.0):\n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.simulations_per_move = simulations_per_move\n",
    "        self.max_depth = max_depth\n",
    "        self.c_puct = c_puct\n",
    "        self.Q = collections.defaultdict(self._zero_array_factory)\n",
    "        self.N_sa = collections.defaultdict(self._zero_array_factory)\n",
    "        self.N_s = collections.defaultdict(int)\n",
    "        self.P = {}\n",
    "\n",
    "    def _get_matrix_state(self, state):\n",
    "        board = state['board']\n",
    "        num_nodes = len(board)\n",
    "        matrix = np.zeros((num_nodes, num_nodes, 4), dtype=np.float32)\n",
    "\n",
    "        np.fill_diagonal(matrix[:, :, 0], board == 1)\n",
    "        np.fill_diagonal(matrix[:, :, 1], board == 2)\n",
    "\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "        for start, end_list in self.env.adj.items():\n",
    "            for end in end_list:\n",
    "                adj_matrix[start - 1, end - 1] = 1\n",
    "        matrix[:, :, 2] = adj_matrix\n",
    "\n",
    "        # Channel 3 - Player turn, matrix filled with 0 for goat and 1 for tiger\n",
    "        matrix[:, :, 3] = state['player_turn']\n",
    "        \n",
    "        return matrix\n",
    "\n",
    "    def search(self, state, depth):\n",
    "        if depth >= self.max_depth:\n",
    "            matrix_state = self._get_matrix_state(state) \n",
    "            _, value = self.network.predict(matrix_state)\n",
    "            return -value[0][0]\n",
    "\n",
    "        state_key = self._get_state_key(state)\n",
    "        if state_key not in self.P:\n",
    "            matrix_state = self._get_matrix_state(state) \n",
    "            policy, value = self.network.predict(matrix_state)\n",
    "            self.P[state_key] = policy[0]\n",
    "            return -value[0][0]\n",
    "            \n",
    "        node_env = self.env.copy(); node_env.board = state['board'].copy(); node_env.player_turn = state['player_turn']; node_env.goats_placed_count = self.env.NUM_GOATS - state['goats_to_place'][0]; node_env.goats_captured_count = state['goats_captured'][0]\n",
    "        best_ucb = -np.inf; best_action = -1\n",
    "        valid_actions = [a for a in range(node_env.action_space.n) if node_env.is_action_valid(a)[0]]\n",
    "        for action in valid_actions:\n",
    "            q_value = self.Q[state_key][action]; ucb = q_value + self.c_puct * self.P[state_key][action] * np.sqrt(self.N_s[state_key]) / (1 + self.N_sa[state_key][action]);\n",
    "            if ucb > best_ucb: best_ucb = ucb; best_action = action\n",
    "        if best_action == -1: return 0\n",
    "        action = best_action\n",
    "        next_state, _, done, info = node_env.step(action)\n",
    "        if done:\n",
    "            winner = info.get('winner', -1); value = 0\n",
    "            if winner != -1: value = 1 if winner == state['player_turn'] else -1\n",
    "        else: value = self.search(next_state, depth + 1)\n",
    "        self.Q[state_key][action] = (self.N_sa[state_key][action] * self.Q[state_key][action] + value) / (self.N_sa[state_key][action] + 1); self.N_sa[state_key][action] += 1; self.N_s[state_key] += 1\n",
    "        return -value\n",
    "\n",
    "    def get_action(self, state, turn_count, training=True):\n",
    "        state_key = self._get_state_key(state)\n",
    "        for _ in range(self.simulations_per_move): self.search(state, 0)\n",
    "        visit_counts = self.N_sa[state_key]\n",
    "        if training:\n",
    "            tau = 1.0 if turn_count < 10 else 0.1\n",
    "            action_probs = visit_counts**(1/tau)\n",
    "            action_probs[np.isnan(action_probs)] = 0\n",
    "            if np.sum(action_probs) > 0: action_probs /= np.sum(action_probs)\n",
    "            else:\n",
    "                valid_actions = [a for a in range(self.env.action_space.n) if self.env.is_action_valid(a)[0]]\n",
    "                action_probs = np.zeros(self.env.action_space.n)\n",
    "                if valid_actions: action_probs[valid_actions] = 1 / len(valid_actions)\n",
    "            action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "        else: action = np.argmax(visit_counts)\n",
    "        policy_target = visit_counts / np.sum(visit_counts) if np.sum(visit_counts) > 0 else np.zeros(self.env.action_space.n)\n",
    "        return action, policy_target\n",
    "    def _get_state_key(self, state):\n",
    "        return (state['board'].tobytes(), state['player_turn'])\n",
    "    def save_mcts_tree(self, file_path):\n",
    "        print(f\"Saving MCTS tree to {file_path}\"); tree_data = {'Q': self.Q, 'N_sa': self.N_sa, 'N_s': self.N_s, 'P': self.P}\n",
    "        with open(file_path, 'wb') as f: pickle.dump(tree_data, f)\n",
    "    def load_mcts_tree(self, file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading MCTS tree from {file_path}\")\n",
    "            with open(file_path, 'rb') as f:\n",
    "                tree_data = pickle.load(f); self.Q = tree_data['Q']; self.N_sa = tree_data['N_sa']; self.N_s = tree_data['N_s']; self.P = tree_data['P']\n",
    "        else: print(\"No existing MCTS tree found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Self Play</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T15:57:06.539769Z",
     "iopub.status.busy": "2025-08-07T15:57:06.539548Z",
     "iopub.status.idle": "2025-08-07T15:58:54.302234Z",
     "shell.execute_reply": "2025-08-07T15:58:54.301526Z",
     "shell.execute_reply.started": "2025-08-07T15:57:06.539754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 15:57:08,337\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=182522)\u001b[0m 2025-08-07 15:57:10.586611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=182522)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=182522)\u001b[0m E0000 00:00:1754582230.608417  182522 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=182522)\u001b[0m E0000 00:00:1754582230.615372  182522 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m I0000 00:00:1754582235.040520  182522 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m /usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m   saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m Loaded model weights from /kaggle/working/alphazero_aadu_pulli_ray.weights.h5\n",
      "\n",
      "==================== ITERATION 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 110.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m Loaded replay buffer from /kaggle/working/replay_buffer.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36m(pid=182523)\u001b[0m 2025-08-07 15:57:16.981159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=182523)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=182523)\u001b[0m E0000 00:00:1754582237.003964  182523 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=182523)\u001b[0m E0000 00:00:1754582237.010801  182523 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(self_play_task pid=182523)\u001b[0m I0000 00:00:1754582240.974561  182523 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13906 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "\u001b[36m(self_play_task pid=182524)\u001b[0m I0000 00:00:1754582240.985054  182524 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13842 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "\u001b[36m(self_play_task pid=182523)\u001b[0m I0000 00:00:1754582242.244398  182787 service.cc:148] XLA service 0x78832c0050e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(self_play_task pid=182523)\u001b[0m I0000 00:00:1754582242.246251  182787 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "\u001b[36m(pid=182524)\u001b[0m 2025-08-07 15:57:16.982766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(self_play_task pid=182523)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(pid=182524)\u001b[0m E0000 00:00:1754582237.005125  182524 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=182524)\u001b[0m E0000 00:00:1754582237.011811  182524 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(self_play_task pid=182523)\u001b[0m I0000 00:00:1754582242.296543  182787 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[36m(self_play_task pid=182523)\u001b[0m I0000 00:00:1754582243.314857  182787 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m I0000 00:00:1754582274.959749  182737 service.cc:148] XLA service 0x79456c00c1d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m I0000 00:00:1754582274.959834  182737 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(self_play_task pid=182524)\u001b[0m I0000 00:00:1754582242.296518  182795 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[36m(self_play_task pid=182524)\u001b[0m I0000 00:00:1754582243.315855  182795 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m I0000 00:00:1754582275.159969  182737 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m I0000 00:00:1754582282.398775  182737 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "==================== ITERATION 2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1893.16it/s]\n",
      "\u001b[36m(pid=182521)\u001b[0m 2025-08-07 15:58:03.944829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=182521)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=182521)\u001b[0m E0000 00:00:1754582283.968560  182521 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=182521)\u001b[0m E0000 00:00:1754582283.976780  182521 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(self_play_task pid=182521)\u001b[0m I0000 00:00:1754582288.499825  182521 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "\u001b[36m(pid=188260)\u001b[0m 2025-08-07 15:58:04.396529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=188260)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=188260)\u001b[0m E0000 00:00:1754582284.438802  188260 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=188260)\u001b[0m E0000 00:00:1754582284.453370  188260 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(self_play_task pid=182521)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(self_play_task pid=182521)\u001b[0m I0000 00:00:1754582289.735908  188373 service.cc:148] XLA service 0x7cc020005ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(self_play_task pid=182521)\u001b[0m I0000 00:00:1754582289.735970  188373 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "\u001b[36m(self_play_task pid=182521)\u001b[0m I0000 00:00:1754582289.775721  188373 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[36m(self_play_task pid=182521)\u001b[0m I0000 00:00:1754582290.581786  188373 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n",
      "Training step complete.\n",
      "\n",
      "Saving training state for long-term learning...\n",
      "Saved model to /kaggle/working/alphazero_aadu_pulli_ray.weights.h5 and buffer to /kaggle/working/replay_buffer.pkl\n",
      "\n",
      "==================== ITERATION 3 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n",
      "Training step complete.\n",
      "\n",
      "==================== ITERATION 4 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "Saving training state for long-term learning...\n",
      "Saved model to /kaggle/working/alphazero_aadu_pulli_ray.weights.h5 and buffer to /kaggle/working/replay_buffer.pkl\n",
      "\n",
      "==================== ITERATION 5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "==================== ITERATION 6 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n",
      "Training step complete.\n",
      "\n",
      "Saving training state for long-term learning...\n",
      "Saved model to /kaggle/working/alphazero_aadu_pulli_ray.weights.h5 and buffer to /kaggle/working/replay_buffer.pkl\n",
      "\n",
      "==================== ITERATION 7 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "==================== ITERATION 8 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "Saving training state for long-term learning...\n",
      "Saved model to /kaggle/working/alphazero_aadu_pulli_ray.weights.h5 and buffer to /kaggle/working/replay_buffer.pkl\n",
      "\n",
      "==================== ITERATION 9 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "==================== ITERATION 10 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time done\n",
      "\u001b[36m(Trainer pid=182522)\u001b[0m Training network on 1024 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step complete.\n",
      "\n",
      "Saving training state for long-term learning...\n",
      "Saved model to /kaggle/working/alphazero_aadu_pulli_ray.weights.h5 and buffer to /kaggle/working/replay_buffer.pkl\n",
      "\n",
      "Training complete. Performing final save...\n",
      "Saved model to /kaggle/working/alphazero_aadu_pulli_ray.weights.h5 and buffer to /kaggle/working/replay_buffer.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(self_play_task pid=188260)\u001b[0m I0000 00:00:1754582288.959029  188260 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13840 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "\u001b[36m(self_play_task pid=188260)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(self_play_task pid=188260)\u001b[0m I0000 00:00:1754582290.102959  188406 service.cc:148] XLA service 0x7f20e0006850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(self_play_task pid=188260)\u001b[0m I0000 00:00:1754582290.103012  188406 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "\u001b[36m(self_play_task pid=188260)\u001b[0m I0000 00:00:1754582290.144306  188406 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[36m(self_play_task pid=188260)\u001b[0m I0000 00:00:1754582290.841156  188406 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# --- Ray-based Distributed AlphaZero Training ---\n",
    "\n",
    "def train_agent(agent, training_data, batch_size=64):\n",
    "    # Trains the neural network using collected self-play data\n",
    "    if not training_data: print(\"Training data is empty, skipping training\"); return\n",
    "    states = np.array([agent._get_matrix_state(d[0]) for d in training_data])\n",
    "    target_policies = np.array([d[1] for d in training_data])\n",
    "    target_values = np.array([d[2] for d in training_data])\n",
    "    agent.network.train(states, target_policies, target_values, batch_size=batch_size)\n",
    "\n",
    "'''\n",
    "Ray's @ray.remote decorator is similar to PySpark's distributed function execution.\n",
    "It turns a Python function into a remote task that can be executed in parallel on different workers.\n",
    "PySpark -> .map() or .foreach() on an RDD/DataFrame to distribute work.\n",
    "@ray.remote distributes self-play games across CPU cores.\n",
    "'''\n",
    "# The @ray.remote decorator turns a normal Python function into a distributed task\n",
    "@ray.remote(num_cpus=1, num_gpus=0.1) # 1 task uses 1 CPU and 0.1 of GPU( meaning it can run 10 tasks concurrently)\n",
    "def self_play_task(model_weights):\n",
    "    # This worker receives the model weights directly, instead of reading from a file\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    env = AaduPulliEnv()\n",
    "    action_space_size = env.action_space.n\n",
    "    \n",
    "    network = NeuralNetwork(action_space_size)\n",
    "    network.model.set_weights(model_weights) \n",
    "\n",
    "    agent = AlphaZeroAgent(env, network, simulations_per_move=SIMULATIONS_PER_MOVE, max_depth=25)\n",
    "    \n",
    "    game_history = []; state = agent.env.reset(); done = False; turn = 0\n",
    "    while not done:\n",
    "        action, policy = agent.get_action(state, turn, training=True)\n",
    "        game_history.append((state, policy)); state, _, done, info = agent.env.step(action); turn += 1\n",
    "    winner = info.get('winner', -1); game_training_data = []\n",
    "    for hist_state, hist_policy in game_history:\n",
    "        value = 0\n",
    "        if winner != -1: value = 1 if hist_state['player_turn'] == winner else -1\n",
    "        game_training_data.append((hist_state, hist_policy, value))\n",
    "    return game_training_data\n",
    "\n",
    "'''\n",
    "Ray actors are similar to PySpark's accumulators or broadcast variables, but with mutable state.\n",
    "A Ray actor is a Python class that runs in its own process and can maintain state across method calls.\n",
    "This is useful for things like model weights and replay buffers.\n",
    "'''\n",
    "@ray.remote(num_gpus=1) # This actor will live on the GPU\n",
    "class Trainer:\n",
    "    def __init__(self, action_space_size, learning_rate, replay_buffer_size):\n",
    "        self.network = NeuralNetwork(action_space_size, learning_rate)\n",
    "        self.agent = AlphaZeroAgent(AaduPulliEnv(), self.network) \n",
    "        self.buffer = collections.deque(maxlen=replay_buffer_size)\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        # Sample a batch from the replay buffer and train the model\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return \"Buffer too small, skipping training.\"\n",
    "        \n",
    "        training_data = random.sample(list(self.buffer), batch_size)\n",
    "        train_agent(self.agent, training_data, batch_size)\n",
    "        return \"Training step complete.\"\n",
    "\n",
    "    def add_data(self, data):\n",
    "        # Add new self-play data to the replay buffer\n",
    "        self.buffer.extend(data)\n",
    "\n",
    "    def get_weights(self):\n",
    "        # Return the current model weights\n",
    "        return self.network.model.get_weights()\n",
    "\n",
    "    def get_buffer_size(self):\n",
    "        # Return the current size of the replay buffer\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def save_state(self, model_path, buffer_path):\n",
    "        # Save model weights and replay buffer to disk\n",
    "        self.network.model.save_weights(model_path)\n",
    "        with open(buffer_path, 'wb') as f:\n",
    "            pickle.dump(self.buffer, f)\n",
    "        return f\"Saved model to {model_path} and buffer to {buffer_path}\"\n",
    "\n",
    "    def load_state(self, model_path, buffer_path):\n",
    "        # Load model weights and replay buffer from disk if available\n",
    "        if os.path.exists(model_path):\n",
    "            self.network.model.load_weights(model_path)\n",
    "            print(f\"Loaded model weights from {model_path}\")\n",
    "        if os.path.exists(buffer_path):\n",
    "            with open(buffer_path, 'rb') as f:\n",
    "                self.buffer = pickle.load(f)\n",
    "            print(f\"Loaded replay buffer from {buffer_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ray.init()  # Initialize Ray runtime (like SparkContext in PySpark)\n",
    "\n",
    "    NUM_ITERATIONS = 10\n",
    "    GAMES_PER_ITERATION = 20\n",
    "    LEARNING_RATE = 0.001\n",
    "    BATCH_SIZE = 1024\n",
    "    REPLAY_BUFFER_SIZE = 50000 \n",
    "    SIMULATIONS_PER_MOVE = 80\n",
    "    MODEL_SAVE_PATH = '...'\n",
    "    BUFFER_SAVE_PATH = '...'\n",
    "    MODEL_IMPORT_PATH = '...'\n",
    "    BUFFER_IMPORT_PATH = '...'\n",
    "    MAX_ELAPSED_SECONDS = 39600\n",
    "\n",
    "    action_space_size = AaduPulliEnv().action_space.n\n",
    "    \n",
    "    # Create the central trainer actor (like a driver node in Spark)\n",
    "    trainer = Trainer.remote(action_space_size, LEARNING_RATE, REPLAY_BUFFER_SIZE)\n",
    "    ray.get(trainer.load_state.remote(MODEL_SAVE_PATH, BUFFER_SAVE_PATH))\n",
    "    time_elapsed = time.time()\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        print(f\"\\n{'='*20} ITERATION {iteration+1} {'='*20}\")\n",
    "        \n",
    "        model_weights_id = trainer.get_weights.remote()\n",
    "\n",
    "        results_ids = []\n",
    "        for _ in tqdm(range(GAMES_PER_ITERATION)):\n",
    "            # In PySpark, this would be like submitting jobs to the cluster.\n",
    "            # Here, we launch remote self-play tasks in parallel.\n",
    "            if (time.time() - time_elapsed)  >= MAX_ELAPSED_SECONDS : print(\"Max seconds - {MAX_ELAPSED_SECONDS} elapsed\"); break\n",
    "                \n",
    "            results_ids.append(self_play_task.remote(model_weights_id))\n",
    "\n",
    "        # Gather results from all remote tasks (like collecting RDD results in PySpark)\n",
    "        all_new_data = ray.get(results_ids)\n",
    "        \n",
    "        for data in all_new_data:\n",
    "            trainer.add_data.remote(data)\n",
    "        train_status = ray.get(trainer.train.remote(BATCH_SIZE))\n",
    "        print(train_status)\n",
    "        \n",
    "        if (iteration + 1) % 2 == 0:\n",
    "            print(\"Saving training state for long-term learning\")\n",
    "            save_message = ray.get(trainer.save_state.remote(MODEL_SAVE_PATH, BUFFER_SAVE_PATH))\n",
    "            print(save_message)\n",
    "    \n",
    "    ray.shutdown()  # Clean up Ray resources (like SparkContext.stop())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 254645733,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
