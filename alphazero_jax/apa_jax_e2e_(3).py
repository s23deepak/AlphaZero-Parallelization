# -*- coding: utf-8 -*-
"""apa-jax-e2e (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1agbJiRL94Uw5IoExXikMnQmRMfyUa8A8
"""
import jax
import jax.numpy as jnp
from jax import lax, random, jit, vmap, pmap

import flax.linen as nn
from flax.training import train_state
from flax import serialization # Or from flax.serialization import to_bytes, from_bytes
from flax import jax_utils

import optax
import mctx

from functools import partial
from typing import Tuple, Dict, NamedTuple
import chex
import time

import numpy as np
import os

import pgx
from pgx.experimental import auto_reset

from clu import metric_writers
from pynvml import *

nvmlInit()
# Get a handle for each GPU
handle0 = nvmlDeviceGetHandleByIndex(0)
handle1 = nvmlDeviceGetHandleByIndex(1)

# --- 1. PGX GAME ENVIRONMENT ---

@chex.dataclass(frozen=True)
class State:
    """
    PGX-compatible state for the Aadu Puli Aattam game.
    """
    current_player: jax.Array  # PGX requires this field
    observation: jax.Array  # Board representation
    rewards: jax.Array  # PGX requires this field (shape: [2,])
    terminated: jax.Array  # PGX requires this field
    truncated: jax.Array  # PGX requires this field
    legal_action_mask: jax.Array  # PGX requires this field
    _step_count: jax.Array  # Internal counter (PGX requires underscore prefix)
    # Game-specific fields
    _board: jax.Array  # The actual board state (23 positions)
    _goats_to_place: jax.Array
    _goats_captured: jax.Array
    
    @property
    def env_id(self):
        return "aadu_puli_aattam"

class AaduPuliAattamJAX(pgx.Env):
    def __init__(self):
        super().__init__()
        # --- Game Constants ---
        self.NUM_GOATS = 15
        self.NUM_TIGERS = 3
        self.TIGER_WIN_THRESHOLD = 10
        self.BOARD_POSITIONS = 23
        self.MAX_TURNS = 200

        # --- Adjacency and Action Mapping Setup (Order is important!) ---
        adj_dict = self._get_adjacency()
        jump_adj_dict = self._get_jump_adjacency()
        self.max_neighbors = max((len(v) for v in adj_dict.values()), default=0)
        self._move_action_map, self._move_action_lookup = self._create_move_maps(adj_dict, jump_adj_dict)
        self.placement_actions_count = self.BOARD_POSITIONS
        self.move_actions_count = len(self._move_action_map)
        self.total_actions = self.placement_actions_count + self.move_actions_count
        self._adj_matrix, self._num_adj = self._create_adj_matrix_from_dict(adj_dict)
        self._jump_adj_matrix, self._num_jump_adj = self._create_adj_matrix_from_dict(jump_adj_dict)


    def _create_adj_matrix_from_dict(self, adj_dict: Dict[int, list]) -> Tuple[chex.Array, chex.Array]:
        """Converts a python adjacency dict to a JAX-compatible padded matrix."""
        matrix = np.zeros((self.BOARD_POSITIONS + 1, self.max_neighbors), dtype=np.int32)
        counts = np.zeros(self.BOARD_POSITIONS + 1, dtype=np.int32)
        for pos, neighbors in adj_dict.items():
            num = len(neighbors)
            matrix[pos, :num] = neighbors
            counts[pos] = num
        return jnp.array(matrix), jnp.array(counts)

    def _get_adjacency(self) -> Dict[int, list]:
        return {
            1: [3, 4, 5, 6], 2: [3, 8], 3: [1, 4, 9, 2], 4: [1, 5, 10, 3], 5: [1, 6, 11, 4], 6: [1, 7, 12, 5], 7: [6, 13],
            8: [2, 9, 14], 9: [3, 10, 15, 8], 10: [4, 11, 16, 9], 11: [5, 12, 17, 10], 12: [6, 13, 18, 11], 13: [7, 14, 12],
            14: [8, 15], 15: [9, 16, 20, 14], 16: [10, 17, 21, 15], 17: [11, 18, 22, 16], 18: [12, 19, 23, 17], 19: [13, 18],
            20: [15, 21], 21: [16, 20, 22], 22: [17, 21, 23], 23: [18, 22]
        }

    def _get_jump_adjacency(self) -> Dict[int, list]:
        return {
            1: [9, 10, 11, 12], 2: [4, 14], 3: [5, 15], 4: [2, 6, 16], 5: [3, 7, 17], 6: [4, 18], 7: [5, 19],
            8: [10], 9: [1, 11, 20], 10: [1, 8, 12, 21], 11: [1, 9, 13, 22], 12: [1, 10, 23], 13: [11],
            14: [2, 16], 15: [3, 17], 16: [4, 14, 18], 17: [5, 15, 19], 18: [6, 16], 19: [7, 17],
            20: [9, 22], 21: [10, 23], 22: [11, 20], 23: [12, 21]
        }

    def _create_move_maps(self, adj_dict: Dict, jump_adj_dict: Dict) -> Tuple[chex.Array, dict]:
        """Creates action mappings from python dictionaries."""
        action_map_list = []
        action_lookup = {}
        index = 0
        for start_pos in range(1, self.BOARD_POSITIONS + 1):
            for end_pos in adj_dict.get(start_pos, []):
                move = (start_pos, end_pos)
                action_map_list.append(move)
                action_lookup[move] = index
                index += 1
            for end_pos in jump_adj_dict.get(start_pos, []):
                move = (start_pos, end_pos)
                if move not in action_lookup:
                    action_map_list.append(move)
                    action_lookup[move] = index
                    index += 1
        return jnp.array(action_map_list, dtype=jnp.int32), action_lookup

    def _init(self, key: jax.Array) -> State:
        """Initialize the game state. PGX uses _init instead of reset."""
        board = jnp.zeros(self.BOARD_POSITIONS, dtype=jnp.int32)
        board = board.at[jnp.array([0, 3, 4])].set(2)
        
        # Create initial state
        init_state = State(
            current_player=jnp.int32(0),  # Goat player starts
            observation=self._make_observation(board, 0, self.NUM_GOATS, 0, 0),
            rewards=jnp.zeros(2, dtype=jnp.float32),
            terminated=jnp.bool_(False),
            truncated=jnp.bool_(False),
            legal_action_mask=jnp.zeros(self.total_actions, dtype=jnp.bool_),
            _step_count=jnp.int32(0),
            _board=board,
            _goats_to_place=jnp.int32(self.NUM_GOATS),
            _goats_captured=jnp.int32(0)
        )
        
        # Compute legal actions for initial state
        legal_mask = self._compute_legal_actions(init_state)
        return init_state.replace(legal_action_mask=legal_mask)

    def _compute_legal_actions(self, state: State) -> chex.Array:
        mask = jnp.zeros(self.total_actions, dtype=jnp.bool_)

        def goat_placement_mask():
            return (state._board == 0).astype(jnp.bool_)

        def goat_move_mask():
            move_mask = jnp.zeros(self.move_actions_count, dtype=jnp.bool_)
            goat_indices = jnp.where(state._board == 1, size=self.NUM_GOATS, fill_value=-1)[0]

            def check_move(i, current_mask):
                from_idx = goat_indices[i]
                from_pos = from_idx + 1

                def update_mask(j, inner_mask):
                    end_pos = self._adj_matrix[from_pos, j]
                    to_idx = end_pos - 1
                    is_empty = (state._board[to_idx] == 0)
                    move_tuple = jnp.array([from_pos, end_pos])
                    action_idx = jnp.where(jnp.all(self._move_action_map == move_tuple, axis=1), size=1, fill_value=-1)[0][0]
                    return lax.cond(
                        (action_idx != -1) & is_empty,
                        lambda: inner_mask.at[action_idx].set(True),
                        lambda: inner_mask
                    )
                num_neighbors = self._num_adj[from_pos]
                return lax.fori_loop(0, num_neighbors, update_mask, current_mask)

            return lax.fori_loop(0, self.NUM_GOATS, check_move, move_mask)

        def tiger_move_mask():
            move_mask = jnp.zeros(self.move_actions_count, dtype=jnp.bool_)
            tiger_indices = jnp.where(state._board == 2, size=self.NUM_TIGERS, fill_value=-1)[0]

            def check_tiger_moves(i, current_mask):
                from_idx = tiger_indices[i]
                from_pos = from_idx + 1

                def check_adj_move(j, inner_mask):
                    end_pos = self._adj_matrix[from_pos, j]
                    to_idx = end_pos - 1
                    is_empty = (state._board[to_idx] == 0)
                    move_tuple = jnp.array([from_pos, end_pos])
                    action_idx = jnp.where(jnp.all(self._move_action_map == move_tuple, axis=1), size=1, fill_value=-1)[0][0]
                    return lax.cond(
                        (action_idx != -1) & is_empty,
                        lambda: inner_mask.at[action_idx].set(True),
                        lambda: inner_mask
                    )
                num_adj = self._num_adj[from_pos]
                current_mask = lax.fori_loop(0, num_adj, check_adj_move, current_mask)

                def check_jump_move(j, inner_mask):
                    end_pos = self._jump_adj_matrix[from_pos, j]
                    to_idx = end_pos - 1
                    is_empty = (state._board[to_idx] == 0)

                    from_neighbors_padded = self._adj_matrix[from_pos]
                    to_neighbors_padded = self._adj_matrix[to_idx + 1]
                    from_mask = jnp.arange(self.max_neighbors) < self._num_adj[from_pos]
                    to_mask = jnp.arange(self.max_neighbors) < self._num_adj[to_idx + 1]
                    from_valid = jnp.where(from_mask, from_neighbors_padded, -1)
                    to_valid = jnp.where(to_mask, to_neighbors_padded, -2)

                    mid_pos_arr = jnp.intersect1d(from_valid, to_valid, size=self.max_neighbors, fill_value=0)

                    def process_jump(inner_mask_):
                        mid_pos = mid_pos_arr[0]
                        mid_idx = mid_pos - 1
                        is_goat_in_middle = (state._board[mid_idx] == 1)
                        move_tuple = jnp.array([from_pos, end_pos])
                        action_idx = jnp.where(jnp.all(self._move_action_map == move_tuple, axis=1), size=1, fill_value=-1)[0][0]
                        return lax.cond(
                            (action_idx != -1) & is_empty & is_goat_in_middle,
                            lambda: inner_mask_.at[action_idx].set(True),
                            lambda: inner_mask_
                        )
                    return lax.cond(mid_pos_arr[0] > 0, process_jump, lambda m: m, inner_mask)

                num_jump = self._num_jump_adj[from_pos]
                current_mask = lax.fori_loop(0, num_jump, check_jump_move, current_mask)
                return current_mask

            return lax.fori_loop(0, self.NUM_TIGERS, check_tiger_moves, move_mask)

        is_goat_turn = state.current_player == 0
        is_placement_phase = state._goats_to_place > 0
        placement_mask = lax.cond(is_goat_turn & is_placement_phase, goat_placement_mask, lambda: jnp.zeros(self.placement_actions_count, dtype=jnp.bool_))
        movement_mask = lax.cond(is_goat_turn & ~is_placement_phase, goat_move_mask, lambda: lax.cond(~is_goat_turn, tiger_move_mask, lambda: jnp.zeros(self.move_actions_count, dtype=jnp.bool_)))
        return jnp.concatenate([placement_mask, movement_mask])

    def _are_tigers_blocked(self, board: chex.Array) -> chex.Array:
        temp_state = State(
            current_player=jnp.int32(1),  # Tiger player
            observation=jnp.zeros((self.BOARD_POSITIONS * 3 + 4,), dtype=jnp.float32),
            rewards=jnp.zeros(2, dtype=jnp.float32),
            terminated=jnp.bool_(False),
            truncated=jnp.bool_(False),
            legal_action_mask=jnp.zeros(self.total_actions, dtype=jnp.bool_),
            _step_count=jnp.int32(0),
            _board=board,
            _goats_to_place=jnp.int32(0),
            _goats_captured=jnp.int32(0)
        )
        mask = self._compute_legal_actions(temp_state)
        return ~jnp.any(mask)

    def _step(self, state: State, action: chex.Array, key: chex.PRNGKey) -> State:
        """PGX step function - takes state, action, and key; returns new state."""
        def perform_action(state, action):
            is_placement = action < self.placement_actions_count

            def place_goat():
                to_idx = action
                new_board = state._board.at[to_idx].set(1)
                new_goats_to_place = state._goats_to_place - 1
                return new_board, new_goats_to_place, state._goats_captured, jnp.array(0.0)

            def move_piece():
                move_idx = action - self.placement_actions_count
                from_pos, to_pos = self._move_action_map[move_idx]
                from_idx, to_idx = from_pos - 1, to_pos - 1
                piece = state._board[from_idx]
                temp_board = state._board.at[from_idx].set(0)
                new_board = temp_board.at[to_idx].set(piece)

                def tiger_jump():
                    from_neighbors_padded = self._adj_matrix[from_pos]
                    to_neighbors_padded = self._adj_matrix[to_pos]
                    from_mask = jnp.arange(self.max_neighbors) < self._num_adj[from_pos]
                    to_mask = jnp.arange(self.max_neighbors) < self._num_adj[to_pos]
                    from_valid = jnp.where(from_mask, from_neighbors_padded, -1)
                    to_valid = jnp.where(to_mask, to_neighbors_padded, -2)

                    mid_pos_arr = jnp.intersect1d(from_valid, to_valid, size=self.max_neighbors, fill_value=0)

                    def capture_goat():
                        mid_idx = mid_pos_arr[0] - 1
                        b = new_board.at[mid_idx].set(0)
                        g_cap = state._goats_captured + 1
                        r = jnp.array(5.0)
                        return b, state._goats_to_place, g_cap, r

                    return lax.cond(
                        mid_pos_arr[0] > 0,
                        capture_goat,
                        lambda: (new_board, state._goats_to_place, state._goats_captured, jnp.array(0.0))
                    )

                final_board, g_place, g_cap, r = lax.cond(
                    piece == 2,
                    tiger_jump,
                    lambda: (new_board, state._goats_to_place, state._goats_captured, jnp.array(0.0))
                )
                return final_board, g_place, g_cap, r

            new_board, new_goats_to_place, new_goats_captured, reward = lax.cond(is_placement, place_goat, move_piece)

            new_step_count = state._step_count + 1
            goat_win = self._are_tigers_blocked(new_board)
            tiger_win = new_goats_captured >= self.TIGER_WIN_THRESHOLD
            draw = new_step_count >= self.MAX_TURNS
            terminated = goat_win | tiger_win | draw

            # Calculate rewards for both players
            # Goat (player 0) gets +1 for win, Tiger (player 1) gets +1 for win
            goat_reward = lax.cond(goat_win, lambda: 1.0, lambda: lax.cond(tiger_win, lambda: -1.0, lambda: 0.0))
            tiger_reward = -goat_reward
            
            # Add intermediate reward (for tiger captures)
            tiger_reward = tiger_reward + lax.cond(state.current_player == 1, lambda: reward, lambda: 0.0)
            
            rewards = jnp.array([goat_reward, tiger_reward], dtype=jnp.float32)
            
            next_player = 1 - state.current_player
            new_obs = self._make_observation(new_board, next_player, new_goats_to_place, new_goats_captured, new_step_count)
            
            next_state = State(
                current_player=next_player,
                observation=new_obs,
                rewards=rewards,
                terminated=terminated,
                truncated=jnp.bool_(False),
                legal_action_mask=jnp.zeros(self.total_actions, dtype=jnp.bool_),
                _step_count=new_step_count,
                _board=new_board,
                _goats_to_place=new_goats_to_place,
                _goats_captured=new_goats_captured
            )
            
            # Compute legal actions for next state
            legal_mask = self._compute_legal_actions(next_state)
            next_state = next_state.replace(legal_action_mask=legal_mask)
            
            return next_state

        return lax.cond(state.terminated, lambda: state, lambda: perform_action(state, action))

    def _make_observation(self, board: chex.Array, player: chex.Array, goats_to_place: chex.Array, 
                          goats_captured: chex.Array, step_count: chex.Array) -> chex.Array:
        """Create observation from game state components."""
        board_one_hot = jax.nn.one_hot(board, 3).flatten()
        state_features = jnp.array([
            player,
            goats_to_place / self.NUM_GOATS,
            goats_captured / self.TIGER_WIN_THRESHOLD,
            step_count / self.MAX_TURNS
        ], dtype=jnp.float32)
        return jnp.concatenate([board_one_hot, state_features])
    
    def _observe(self, state: State, player_id: chex.Array) -> chex.Array:
        """PGX observe method - returns observation for a specific player."""
        # Return the stored observation (already computed in state)
        return state.observation
    
    @property
    def id(self) -> str:
        """PGX required property: environment ID."""
        return "aadu_puli_aattam"
    
    @property
    def version(self) -> str:
        """PGX required property: environment version."""
        return "v0"
    
    @property
    def num_players(self) -> int:
        """PGX required property: number of players."""
        return 2
    
    @property
    def observation_shape(self):
        """Return the shape of observations."""
        return (self.BOARD_POSITIONS * 3 + 4,)
    
    @property  
    def num_actions(self):
        """Return the number of possible actions."""
        return self.total_actions


# --- 2. NEURAL NETWORK (FLAX) ---
class AlphaZeroNet(nn.Module):
    action_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(features=256)(x)
        x = nn.relu(x)
        x = nn.Dense(features=256)(x)
        x = nn.relu(x)
        policy_logits = nn.Dense(features=self.action_size)(x)
        value_logit = nn.Dense(features=1)(x)
        value = nn.tanh(value_logit)
        return policy_logits, jnp.squeeze(value, axis=-1)

# --- 3. MCTS and TRAINING SETUP ---
class TrainState(train_state.TrainState):
    pass

def create_train_state(rng, learning_rate, action_size, obs_shape):
    model = AlphaZeroNet(action_size=action_size)
    params = model.init(rng, jnp.ones(obs_shape))['params']
    tx = optax.adam(learning_rate)
    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)

@partial(jit, static_argnums=(2,))
def get_model_outputs(state: TrainState, obs: chex.Array, action_size: int):
    policy_logits, value = state.apply_fn({'params': state.params}, obs)
    return policy_logits, value

# --- 4. THE SELF-PLAY AND TRAINING LOOP ---
def run_self_play_and_train(
    key: chex.PRNGKey,
    train_state: TrainState,
    env: AaduPuliAattamJAX,
    num_simulations: int,
    batch_size: int,
    game_length: int
):

    @jit
    def single_game_step(state_and_key, _):
        state, key = state_and_key
        current_obs = state.observation
        policy_logits, value = get_model_outputs(train_state, current_obs, env.num_actions)
        legal_mask = state.legal_action_mask

        batched_logits = jnp.expand_dims(policy_logits, axis=0)
        batched_value = jnp.expand_dims(value, axis=0)

        batched_embedding_state = jax.tree.map(lambda x: jnp.expand_dims(x, axis=0), state)

        root = mctx.RootFnOutput(
            prior_logits=jnp.where(jnp.expand_dims(legal_mask, axis=0), batched_logits, -jnp.inf),
            value=batched_value,
            embedding=batched_embedding_state
        )

        def recurrent_fn(params, rng_key, action, embedding_state):
            # --- FIX: Un-batch the state from MCTS before passing to env ---
            unbatched_state = jax.tree.map(lambda x: jnp.squeeze(x, axis=0), embedding_state)

            next_state = env.step(unbatched_state, action[0])
            next_obs = next_state.observation
            logits, val = get_model_outputs(train_state, next_obs, env.num_actions)
            
            # Get reward for current player
            reward = next_state.rewards[unbatched_state.current_player]
            discount = jnp.array(1.0, dtype=jnp.float32) * (1 - next_state.terminated)

            recurrent_fn_output = mctx.RecurrentFnOutput(
                reward=jnp.expand_dims(reward, axis=0),
                discount=jnp.expand_dims(discount, axis=0),
                prior_logits=jnp.where(jnp.expand_dims(next_state.legal_action_mask, axis=0), jnp.expand_dims(logits, axis=0), -jnp.inf),
                value=jnp.expand_dims(val, axis=0)
            )
            # Re-batch the next_state before returning it as the new embedding.
            batched_next_state = jax.tree.map(lambda x: jnp.expand_dims(x, axis=0), next_state)

            return recurrent_fn_output, batched_next_state

        policy_output = mctx.muzero_policy(
            params=train_state.params,
            rng_key=key,
            root=root,
            recurrent_fn=recurrent_fn,
            num_simulations=num_simulations,
        )

        action = jnp.squeeze(policy_output.action, axis=0)
        action_weights = jnp.squeeze(policy_output.action_weights, axis=0)

        next_state = env.step(state, action)
        search_pi = jax.nn.softmax(action_weights)
        mcts_value = jnp.squeeze(policy_output.search_tree.summary().value, axis=0)

        data = {'obs': current_obs, 'pi': search_pi, 'value': mcts_value}
        key, _ = random.split(key)
        return (next_state, key), data

    # Use PGX's vectorized init
    keys = random.split(key, batch_size)
    init_fn = jax.vmap(env.init)
    vmapped_initial_state = init_fn(keys)
    (_, _), trajectory_data = lax.scan(vmap(single_game_step), (vmapped_initial_state, keys), None, length=game_length)

    obs_batch = jnp.reshape(trajectory_data['obs'], (-1, *trajectory_data['obs'].shape[2:]))
    pi_batch = jnp.reshape(trajectory_data['pi'], (-1, *trajectory_data['pi'].shape[2:]))
    value_batch = jnp.reshape(trajectory_data['value'], (-1,))

    @jit
    def train_step(state, batch):
        obs, target_pi, target_value = batch
        def loss_fn(params):
            policy_logits, pred_value = state.apply_fn({'params': params}, obs)
            value_loss = jnp.mean((pred_value - target_value) ** 2)
            policy_loss = -jnp.mean(jnp.sum(target_pi * jax.nn.log_softmax(policy_logits), axis=-1))
            total_loss = value_loss + policy_loss
            return total_loss, (value_loss, policy_loss)

        (loss, (v_loss, p_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
        grads = lax.pmean(grads, axis_name='devices')
        new_state = state.apply_gradients(grads=grads)
        metrics = {'total_loss': loss, 'value_loss': v_loss, 'policy_loss': p_loss}
        return new_state, metrics

    new_train_state, metrics = train_step(train_state, (obs_batch, pi_batch, value_batch))
    return new_train_state, metrics

def save_checkpoint(train_state, path):
  """Saves the training state to a file."""
  # Ensure the directory exists
  os.makedirs(os.path.dirname(path), exist_ok=True)
  # Get the raw bytes of the train_state
  bytes_output = serialization.to_bytes(train_state)
  # Write the bytes to a file
  with open(path, "wb") as f:
    f.write(bytes_output)
  # print(f"Checkpoint saved to {path}")

def load_checkpoint(train_state_shape, path):
    """Loads the training state from a file."""
    with open(path, "rb") as f:
        bytes_input = f.read()
    # Restore the state from bytes, using the shape of an empty state as a template
    return serialization.from_bytes(train_state_shape, bytes_input)

log_dir = '/kaggle/working/logs'
writer = metric_writers.create_default_writer(log_dir)


# --- 5. MAIN EXECUTION ---
if __name__ == '__main__':
    # --- Config ---
    NUM_DEVICES = len(jax.devices())
    BATCH_SIZE = 512 # Per device
    GAME_LENGTH_PER_ITER = 10
    NUM_SIMULATIONS = 32
    LEARNING_RATE = 1e-3
    TRAINING_STEPS = 5000
    STEP_UPDATE = 100

    print(f"--- AlphaZero on Aadu Puli Aattam ---")
    print(f"Detected {NUM_DEVICES} JAX devices: {[d.platform for d in jax.devices()]}")
    if NUM_DEVICES == 0 or 'gpu' not in jax.devices()[0].platform.lower():
        print("WARNING: No GPU detected. Training will be very slow.")

    # --- Initialization ---
    env = AaduPuliAattamJAX()
    key = random.PRNGKey(42)
    init_key, key = random.split(key)
    init_state = env.init(init_key)
    obs_shape = init_state.observation.shape
    initial_train_state = create_train_state(key, LEARNING_RATE, env.num_actions, obs_shape)
    replicated_train_state = jax_utils.replicate(initial_train_state)

    pmapped_main_fn = pmap(
        partial(
            run_self_play_and_train,
            env=env,
            num_simulations=NUM_SIMULATIONS,
            batch_size=BATCH_SIZE,
            game_length=GAME_LENGTH_PER_ITER
        ),
        axis_name='devices'
    )

    print("\nStarting training...")
    total_batch_size = BATCH_SIZE * max(1, NUM_DEVICES)
    print(f"Total batch size across all devices: {total_batch_size}")
    print(f"Data generated per iteration: {total_batch_size * GAME_LENGTH_PER_ITER} state-action pairs")

    for step in range(TRAINING_STEPS):
        start_time = time.time()

        step_keys = random.split(key, max(1, NUM_DEVICES))
        key, _ = random.split(key)

        replicated_train_state, metrics = pmapped_main_fn(step_keys, replicated_train_state)
        metrics = jax_utils.unreplicate(metrics)

        end_time = time.time()

        if step % STEP_UPDATE == 0:
            print(
                f"Step: {step}, "
                f"Time/iter: {end_time - start_time:.2f}s, "
                f"Total Loss: {metrics['total_loss']:.4f}, "
                f"Value Loss: {metrics['value_loss']:.4f}, "
                f"Policy Loss: {metrics['policy_loss']:.4f}"
            )

            # 2. Get GPU stats for both GPUs
            utilization0 = nvmlDeviceGetUtilizationRates(handle0)
            memory0 = nvmlDeviceGetMemoryInfo(handle0)
            utilization1 = nvmlDeviceGetUtilizationRates(handle1)
            memory1 = nvmlDeviceGetMemoryInfo(handle1)

            # --- Log Everything to TensorBoard ---
            writer.write_scalars(step, {
                'Loss/Total': metrics['total_loss'],
                'Loss/Value': metrics['value_loss'],
                'Loss/Policy': metrics['policy_loss'],
                'GPU/GPU_0_Utilization_Percent': utilization0.gpu,
                'GPU/GPU_0_Memory_Used_MiB': memory0.used / 1024**2,
                'GPU/GPU_1_Utilization_Percent': utilization1.gpu,
                'GPU/GPU_1_Memory_Used_MiB': memory1.used / 1024**2
            })
            writer.flush() # Ensure data is written to disk

            # Target the correct variable: replicated_train_state
            state_to_save = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], replicated_train_state))

            # Use the save_checkpoint function defined previously
            save_checkpoint(state_to_save, f'/kaggle/working/apa_jax_e2e.ckpt')

    nvmlShutdown()

# # Load the extension if you haven't already
# %load_ext tensorboard

# # Launch TensorBoard and bind it to all interfaces
# %tensorboard --logdir /kaggle/working/logs --bind_all

"""1. Asynchronous GPU Execution
The key reason is that JAX and your GPUs operate asynchronously.
Think of your CPU as a manager and your GPUs as two highly-specialized workers.
The CPU's job is to quickly prepare batches of work (game states) and dispatch them to the GPUs.
The GPU's job is to perform the heavy lifting (running the neural network and MCTS).
When you call your p_train_step function, the CPU essentially hands off a huge chunk of work to the GPUs and can immediately start preparing the next chunk. It doesn't sit and wait.

2. The Real Bottleneck: Data Transfer
The only time you can slow things down is when you force the CPU to wait for the GPU. This happens specifically when you transfer data back from the GPU's memory to the CPU's memory. In JAX, this is done with jax.device_get().
Heavy Work (on GPU): Running p_train_step for one iteration. Takes ~2-3 seconds.
Light Work (on CPU): Querying GPU usage with pynvml or writing a few numbers to a log file. Takes microseconds or milliseconds.
The "Stop Sign": Calling jax.device_get(total_loss). This forces the entire program to halt until the GPU has finished its 2-3 second task and sent the final loss value back to the CPU.

3. Best Practices for High-Performance Logging
Because the "stop sign" is the only real performance cost, the solution is simple: don't stop very often.
Your current code already follows this principle! You print an update only every 10 steps. This is the correct way to do it.
Bad: Logging every step (introduces frequent, small stalls)
Python

# Don't do this
for i in range(num_steps):
    train_state, (total_loss, v_loss, p_loss) = p_train_step(...)
    
    # This forces a GPU->CPU sync on EVERY iteration
    writer.write_scalars(i, {'total_loss': jax.device_get(total_loss[0])})
Good: Logging every N steps (what you are already doing)
Python

# This is the right way!
for i in range(num_steps):
    train_state, (total_loss, v_loss, p_loss) = p_train_step(...)
    
    # Only sync and log occasionally
    if i % 10 == 0:
        writer.write_scalars(i, {'total_loss': jax.device_get(total_loss[0])})
        # You could also log GPU usage here
By logging infrequently, you allow the GPUs to work through their queues uninterrupted for 9 steps, and you only pay the small synchronization penalty on the 10th step. Given that your iterations take seconds, the few milliseconds spent logging are completely negligible.

In summary: Go ahead and add the tracking. Your intuition to be cautious is spot-on, but the asynchronous nature of JAX means that as long as you limit your use of jax.device_get(), your performance will remain excellent.
"""