{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c8618e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:29:12.929869Z",
     "iopub.status.busy": "2025-09-15T07:29:12.929534Z",
     "iopub.status.idle": "2025-09-15T07:29:13.092198Z",
     "shell.execute_reply": "2025-09-15T07:29:13.091021Z"
    },
    "papermill": {
     "duration": 0.167918,
     "end_time": "2025-09-15T07:29:13.093678",
     "exception": false,
     "start_time": "2025-09-15T07:29:12.925760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\r\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\r\n",
      "Cuda compilation tools, release 12.5, V12.5.82\r\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc274653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:29:13.099578Z",
     "iopub.status.busy": "2025-09-15T07:29:13.099009Z",
     "iopub.status.idle": "2025-09-15T07:30:04.245871Z",
     "shell.execute_reply": "2025-09-15T07:30:04.244782Z"
    },
    "papermill": {
     "duration": 51.151017,
     "end_time": "2025-09-15T07:30:04.247290",
     "exception": false,
     "start_time": "2025-09-15T07:29:13.096273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "ibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\r\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\r\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\r\n",
      "langchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.3 which is incompatible.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q mctx clu pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56591d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:30:04.254852Z",
     "iopub.status.busy": "2025-09-15T07:30:04.254307Z",
     "iopub.status.idle": "2025-09-15T07:30:53.715547Z",
     "shell.execute_reply": "2025-09-15T07:30:53.714746Z"
    },
    "papermill": {
     "duration": 49.466593,
     "end_time": "2025-09-15T07:30:53.717082",
     "exception": false,
     "start_time": "2025-09-15T07:30:04.250489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.2/644.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.13.0.50 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Example for CUDA 12\n",
    "!pip install -q -U \"jax[cuda12]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66222eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax, random, jit, vmap, pmap\n",
    "\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax import serialization # Model serialization\n",
    "from flax import jax_utils # Multi-device utils\n",
    "\n",
    "import optax\n",
    "import mctx\n",
    "\n",
    "from functools import partial\n",
    "from typing import Tuple, Dict\n",
    "import chex\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from clu import metric_writers\n",
    "from pynvml import *\n",
    "\n",
    "nvmlInit()\n",
    "handle0 = nvmlDeviceGetHandleByIndex(0)\n",
    "handle1 = nvmlDeviceGetHandleByIndex(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroNet(nn.Module):\n",
    "    action_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        policy_logits = nn.Dense(features=self.action_size)(x)\n",
    "        value_logit = nn.Dense(features=1)(x)\n",
    "        value = nn.tanh(value_logit)\n",
    "        return policy_logits, jnp.squeeze(value, axis=-1)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, learning_rate, action_size, obs_shape):\n",
    "    model = AlphaZeroNet(action_size=action_size)\n",
    "    params = model.init(rng, jnp.ones(obs_shape))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def get_model_outputs(state: TrainState, obs: chex.Array, action_size: int):\n",
    "    policy_logits, value = state.apply_fn({'params': state.params}, obs)\n",
    "    return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_play_and_train(\n",
    "    key: chex.PRNGKey,\n",
    "    train_state: TrainState,\n",
    "    env: AaduPuliAattamJAX,\n",
    "    num_simulations: int,\n",
    "    batch_size: int,\n",
    "    game_length: int\n",
    "):\n",
    "    \n",
    "    @jit\n",
    "    def single_game_step(state_and_key, _):\n",
    "        state, key = state_and_key\n",
    "        current_obs = env.get_observation(state)\n",
    "        policy_logits, value = get_model_outputs(train_state, current_obs, env.total_actions)\n",
    "        legal_mask = env.legal_action_mask(state)\n",
    "\n",
    "        batched_logits = jnp.expand_dims(policy_logits, axis=0)\n",
    "        batched_value = jnp.expand_dims(value, axis=0)\n",
    "        \n",
    "        batched_embedding_state = jax.tree.map(lambda x: jnp.expand_dims(x, axis=0), state)\n",
    "\n",
    "        root = mctx.RootFnOutput(\n",
    "            prior_logits=jnp.where(jnp.expand_dims(legal_mask, axis=0), batched_logits, -jnp.inf),\n",
    "            value=batched_value,\n",
    "            embedding=batched_embedding_state\n",
    "        )\n",
    "\n",
    "        def recurrent_fn(params, rng_key, action, embedding_state):\n",
    "            unbatched_state = jax.tree.map(lambda x: jnp.squeeze(x, axis=0), embedding_state)\n",
    "            \n",
    "            next_state, reward = env.step(unbatched_state, action[0])\n",
    "            next_obs = env.get_observation(next_state)\n",
    "            logits, val = get_model_outputs(train_state, next_obs, env.total_actions)\n",
    "            discount = jnp.array(1.0, dtype=jnp.float32) * (1 - next_state.terminated)\n",
    "\n",
    "            recurrent_fn_output = mctx.RecurrentFnOutput(\n",
    "                reward=jnp.expand_dims(reward, axis=0),\n",
    "                discount=jnp.expand_dims(discount, axis=0),\n",
    "                prior_logits=jnp.where(jnp.expand_dims(env.legal_action_mask(next_state), axis=0), jnp.expand_dims(logits, axis=0), -jnp.inf),\n",
    "                value=jnp.expand_dims(val, axis=0)\n",
    "            )\n",
    "            batched_next_state = jax.tree.map(lambda x: jnp.expand_dims(x, axis=0), next_state)\n",
    "\n",
    "            return recurrent_fn_output, batched_next_state\n",
    "            \n",
    "        policy_output = mctx.muzero_policy(\n",
    "            params=train_state.params,\n",
    "            rng_key=key,\n",
    "            root=root,\n",
    "            recurrent_fn=recurrent_fn,\n",
    "            num_simulations=num_simulations,\n",
    "        )\n",
    "        \n",
    "        action = jnp.squeeze(policy_output.action, axis=0)\n",
    "        action_weights = jnp.squeeze(policy_output.action_weights, axis=0)\n",
    "        \n",
    "        next_state, _ = env.step(state, action)\n",
    "        search_pi = jax.nn.softmax(action_weights)\n",
    "        mcts_value = jnp.squeeze(policy_output.search_tree.summary().value, axis=0)\n",
    "        \n",
    "        data = {'obs': current_obs, 'pi': search_pi, 'value': mcts_value}\n",
    "        key, _ = random.split(key)\n",
    "        return (next_state, key), data\n",
    "\n",
    "    keys = random.split(key, batch_size)\n",
    "    # Parallelize the environment reset across the batch using vmap\n",
    "    # This creates batch_size independent game instances, each starting from the initial state\n",
    "    # vmap applies the reset function vectorized over the batch dimension\n",
    "    vmapped_initial_state = vmap(lambda _: env.reset())(jnp.arange(batch_size))\n",
    "\n",
    "    # Run the self-play loop in parallel across the batch\n",
    "    # lax.scan iterates over game_length steps, with vmap applying single_game_step to each game in the batch\n",
    "    # This parallelizes the game stepping across all batch_size games simultaneously\n",
    "    (_, _), trajectory_data = lax.scan(vmap(single_game_step), (vmapped_initial_state, keys), None, length=game_length)\n",
    "    \n",
    "    obs_batch = jnp.reshape(trajectory_data['obs'], (-1, *trajectory_data['obs'].shape[2:]))\n",
    "    pi_batch = jnp.reshape(trajectory_data['pi'], (-1, *trajectory_data['pi'].shape[2:]))\n",
    "    value_batch = jnp.reshape(trajectory_data['value'], (-1,))\n",
    "\n",
    "    @jit\n",
    "    def train_step(state, batch):\n",
    "        obs, target_pi, target_value = batch\n",
    "        def loss_fn(params):\n",
    "            policy_logits, pred_value = state.apply_fn({'params': params}, obs)\n",
    "            value_loss = jnp.mean((pred_value - target_value) ** 2)\n",
    "            policy_loss = -jnp.mean(jnp.sum(target_pi * jax.nn.log_softmax(policy_logits), axis=-1))\n",
    "            total_loss = value_loss + policy_loss\n",
    "            return total_loss, (value_loss, policy_loss)\n",
    "            \n",
    "        (loss, (v_loss, p_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "\n",
    "        # Average gradients across all devices to synchronize the model updates\n",
    "        # lax.pmean computes the mean of gradients across the 'devices' axis\n",
    "        # This ensures all devices contribute equally to the parameter updates in data-parallel training\n",
    "        grads = lax.pmean(grads, axis_name='devices')\n",
    "        new_state = state.apply_gradients(grads=grads)\n",
    "        metrics = {'total_loss': loss, 'value_loss': v_loss, 'policy_loss': p_loss}\n",
    "        return new_state, metrics\n",
    "\n",
    "    new_train_state, metrics = train_step(train_state, (obs_batch, pi_batch, value_batch))\n",
    "    return new_train_state, metrics\n",
    "\n",
    "def save_checkpoint(train_state, path):\n",
    "  os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "  bytes_output = serialization.to_bytes(train_state)\n",
    "  with open(path, \"wb\") as f:\n",
    "    f.write(bytes_output)\n",
    "  # print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "def load_checkpoint(train_state_shape, path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        bytes_input = f.read()\n",
    "    return serialization.from_bytes(train_state_shape, bytes_input)\n",
    "\n",
    "log_dir = '/kaggle/working/logs'\n",
    "writer = metric_writers.create_default_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cda1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:30:53.743191Z",
     "iopub.status.busy": "2025-09-15T07:30:53.742437Z",
     "iopub.status.idle": "2025-09-15T10:02:16.028888Z",
     "shell.execute_reply": "2025-09-15T10:02:16.027996Z"
    },
    "papermill": {
     "duration": 9082.30164,
     "end_time": "2025-09-15T10:02:16.030574",
     "exception": false,
     "start_time": "2025-09-15T07:30:53.728934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 07:30:57.991534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757921458.352387      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757921458.459806      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "I0000 00:00:1757921478.019183      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1757921478.020031      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "INFO:2025-09-15 07:31:18,498:jax._src.xla_bridge:822: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AlphaZero on Aadu Puli Aattam ---\n",
      "Detected 2 JAX devices: ['gpu', 'gpu']\n",
      "\n",
      "Starting training...\n",
      "Total batch size across all devices: 1024\n",
      "Data generated per iteration: 10240 state-action pairs\n",
      "Step: 0, Time/iter: 18.34s, Total Loss: 170.8706, Value Loss: 165.7854, Policy Loss: 5.0852\n",
      "Step: 100, Time/iter: 3.20s, Total Loss: 76.7580, Value Loss: 71.7142, Policy Loss: 5.0438\n",
      "Step: 200, Time/iter: 2.89s, Total Loss: 54.0544, Value Loss: 49.0128, Policy Loss: 5.0416\n",
      "Step: 300, Time/iter: 2.25s, Total Loss: 42.8682, Value Loss: 37.8275, Policy Loss: 5.0407\n",
      "Step: 400, Time/iter: 2.10s, Total Loss: 35.6216, Value Loss: 30.5811, Policy Loss: 5.0405\n",
      "Step: 500, Time/iter: 2.03s, Total Loss: 33.3305, Value Loss: 28.2901, Policy Loss: 5.0404\n",
      "Step: 600, Time/iter: 2.26s, Total Loss: 31.8519, Value Loss: 26.8114, Policy Loss: 5.0404\n",
      "Step: 700, Time/iter: 1.94s, Total Loss: 30.3678, Value Loss: 25.3274, Policy Loss: 5.0404\n",
      "Step: 800, Time/iter: 2.00s, Total Loss: 30.1098, Value Loss: 25.0695, Policy Loss: 5.0404\n",
      "Step: 900, Time/iter: 1.72s, Total Loss: 30.0786, Value Loss: 25.0383, Policy Loss: 5.0403\n",
      "Step: 1000, Time/iter: 2.01s, Total Loss: 29.9896, Value Loss: 24.9493, Policy Loss: 5.0403\n",
      "Step: 1100, Time/iter: 1.65s, Total Loss: 30.0293, Value Loss: 24.9891, Policy Loss: 5.0402\n",
      "Step: 1200, Time/iter: 1.61s, Total Loss: 29.9981, Value Loss: 24.9579, Policy Loss: 5.0402\n",
      "Step: 1300, Time/iter: 1.66s, Total Loss: 30.0701, Value Loss: 25.0300, Policy Loss: 5.0402\n",
      "Step: 1400, Time/iter: 1.62s, Total Loss: 30.0855, Value Loss: 25.0454, Policy Loss: 5.0401\n",
      "Step: 1500, Time/iter: 1.67s, Total Loss: 30.1093, Value Loss: 25.0692, Policy Loss: 5.0401\n",
      "Step: 1600, Time/iter: 1.68s, Total Loss: 30.0914, Value Loss: 25.0513, Policy Loss: 5.0401\n",
      "Step: 1700, Time/iter: 1.66s, Total Loss: 30.0631, Value Loss: 25.0230, Policy Loss: 5.0401\n",
      "Step: 1800, Time/iter: 1.65s, Total Loss: 30.1002, Value Loss: 25.0601, Policy Loss: 5.0401\n",
      "Step: 1900, Time/iter: 1.64s, Total Loss: 30.0286, Value Loss: 24.9885, Policy Loss: 5.0401\n",
      "Step: 2000, Time/iter: 1.63s, Total Loss: 30.0923, Value Loss: 25.0522, Policy Loss: 5.0401\n",
      "Step: 2100, Time/iter: 1.59s, Total Loss: 30.1285, Value Loss: 25.0884, Policy Loss: 5.0401\n",
      "Step: 2200, Time/iter: 1.70s, Total Loss: 30.0799, Value Loss: 25.0398, Policy Loss: 5.0401\n",
      "Step: 2300, Time/iter: 1.60s, Total Loss: 30.1114, Value Loss: 25.0714, Policy Loss: 5.0401\n",
      "Step: 2400, Time/iter: 1.58s, Total Loss: 30.0632, Value Loss: 25.0231, Policy Loss: 5.0401\n",
      "Step: 2500, Time/iter: 1.69s, Total Loss: 30.0487, Value Loss: 25.0086, Policy Loss: 5.0401\n",
      "Step: 2600, Time/iter: 1.69s, Total Loss: 30.1551, Value Loss: 25.1150, Policy Loss: 5.0401\n",
      "Step: 2700, Time/iter: 1.68s, Total Loss: 30.1109, Value Loss: 25.0709, Policy Loss: 5.0401\n",
      "Step: 2800, Time/iter: 1.62s, Total Loss: 30.1264, Value Loss: 25.0863, Policy Loss: 5.0400\n",
      "Step: 2900, Time/iter: 1.65s, Total Loss: 30.0622, Value Loss: 25.0221, Policy Loss: 5.0400\n",
      "Step: 3000, Time/iter: 1.64s, Total Loss: 30.0910, Value Loss: 25.0509, Policy Loss: 5.0401\n",
      "Step: 3100, Time/iter: 1.72s, Total Loss: 30.0613, Value Loss: 25.0212, Policy Loss: 5.0400\n",
      "Step: 3200, Time/iter: 1.65s, Total Loss: 30.1034, Value Loss: 25.0633, Policy Loss: 5.0401\n",
      "Step: 3300, Time/iter: 1.63s, Total Loss: 30.1511, Value Loss: 25.1111, Policy Loss: 5.0400\n",
      "Step: 3400, Time/iter: 1.59s, Total Loss: 30.1100, Value Loss: 25.0699, Policy Loss: 5.0400\n",
      "Step: 3500, Time/iter: 1.61s, Total Loss: 30.0725, Value Loss: 25.0324, Policy Loss: 5.0400\n",
      "Step: 3600, Time/iter: 1.59s, Total Loss: 30.1107, Value Loss: 25.0707, Policy Loss: 5.0400\n",
      "Step: 3700, Time/iter: 1.74s, Total Loss: 30.1218, Value Loss: 25.0818, Policy Loss: 5.0400\n",
      "Step: 3800, Time/iter: 1.72s, Total Loss: 30.1291, Value Loss: 25.0891, Policy Loss: 5.0400\n",
      "Step: 3900, Time/iter: 1.58s, Total Loss: 30.0735, Value Loss: 25.0335, Policy Loss: 5.0400\n",
      "Step: 4000, Time/iter: 1.66s, Total Loss: 30.0859, Value Loss: 25.0458, Policy Loss: 5.0400\n",
      "Step: 4100, Time/iter: 1.59s, Total Loss: 30.0779, Value Loss: 25.0379, Policy Loss: 5.0400\n",
      "Step: 4200, Time/iter: 1.66s, Total Loss: 30.1102, Value Loss: 25.0702, Policy Loss: 5.0400\n",
      "Step: 4300, Time/iter: 1.70s, Total Loss: 30.1867, Value Loss: 25.1467, Policy Loss: 5.0400\n",
      "Step: 4400, Time/iter: 1.70s, Total Loss: 30.0922, Value Loss: 25.0522, Policy Loss: 5.0400\n",
      "Step: 4500, Time/iter: 1.80s, Total Loss: 30.1857, Value Loss: 25.1457, Policy Loss: 5.0400\n",
      "Step: 4600, Time/iter: 1.67s, Total Loss: 30.2328, Value Loss: 25.1928, Policy Loss: 5.0400\n",
      "Step: 4700, Time/iter: 1.77s, Total Loss: 30.1863, Value Loss: 25.1463, Policy Loss: 5.0400\n",
      "Step: 4800, Time/iter: 1.75s, Total Loss: 30.1715, Value Loss: 25.1315, Policy Loss: 5.0400\n",
      "Step: 4900, Time/iter: 1.69s, Total Loss: 30.2294, Value Loss: 25.1894, Policy Loss: 5.0400\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. jax_utils.replicate-> Distributes model parameters across devices\n",
    "    # 2. pmap-> Runs the training function in parallel across devices\n",
    "    # 3. Each device processes its own batch of games independently\n",
    "    # 4. Gradients are averaged across devices using lax.pmean\n",
    "    # 5. jax_utils.unreplicate-> Collects metrics from all devices\n",
    "    # 6. jax.device_get-> Transfers state from device to host for saving\n",
    "    NUM_DEVICES = len(jax.devices())\n",
    "    BATCH_SIZE = 512 # Per device\n",
    "    GAME_LENGTH_PER_ITER = 10\n",
    "    NUM_SIMULATIONS = 32\n",
    "    LEARNING_RATE = 1e-3\n",
    "    TRAINING_STEPS = 5000\n",
    "    STEP_UPDATE = 100\n",
    "    print(\"--- AlphaZero on Aadu Puli Aattam ---\")\n",
    "    print(f\"Detected {NUM_DEVICES} JAX devices: {[d.platform for d in jax.devices()]}\")\n",
    "    if NUM_DEVICES == 0 or 'gpu' not in jax.devices()[0].platform.lower():\n",
    "        print(\"WARNING: No GPU detected. Training will be very slow.\")\n",
    "    \n",
    "    env = AaduPuliAattamJAX()\n",
    "    key = random.PRNGKey(42)\n",
    "    obs_shape = env.get_observation(env.reset()).shape\n",
    "    initial_train_state = create_train_state(key, LEARNING_RATE, env.total_actions, obs_shape)\n",
    "\n",
    "    # Replicate the training state across all available devices (GPUs)\n",
    "    # This creates a copy of the model parameters on each device for parallel training\n",
    "    # jax_utils.replicate distributes the single-device state to all devices\n",
    "    replicated_train_state = jax_utils.replicate(initial_train_state)\n",
    "\n",
    "    # pmap creates a parallel version of the training function\n",
    "    # pmap applies the function across devices in parallel, with axis_name='devices' for cross-device communication\n",
    "    # This enables data parallelism, that is, each device runs the self-play and training on its own batch\n",
    "    pmapped_main_fn = pmap(\n",
    "        partial(\n",
    "            run_self_play_and_train,\n",
    "            env=env,\n",
    "            num_simulations=NUM_SIMULATIONS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            game_length=GAME_LENGTH_PER_ITER\n",
    "        ),\n",
    "        axis_name='devices'\n",
    "    )\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    total_batch_size = BATCH_SIZE * max(1, NUM_DEVICES)\n",
    "    print(f\"Total batch size across all devices: {total_batch_size}\")\n",
    "    print(f\"Data generated per iteration: {total_batch_size * GAME_LENGTH_PER_ITER} state-action pairs\")\n",
    "    \n",
    "    for step in range(TRAINING_STEPS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        step_keys = random.split(key, max(1, NUM_DEVICES))\n",
    "        key, _ = random.split(key)\n",
    "        \n",
    "        # Execute the parallel training function across all devices\n",
    "        # Each device runs self-play and training on its own batch of games\n",
    "        # The function returns updated states and metrics from each device\n",
    "        replicated_train_state, metrics = pmapped_main_fn(step_keys, replicated_train_state)\n",
    "        \n",
    "        # Unreplicate the metrics from device-parallel format back to single-device format\n",
    "        # This gathers the metrics from all devices and combines them (e.g., averaging losses)\n",
    "        # jax_utils.unreplicate converts from device-distributed to host format\n",
    "        metrics = jax_utils.unreplicate(metrics)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        if step % STEP_UPDATE == 0:\n",
    "            print(\n",
    "                f\"Step: {step}, \"\n",
    "                f\"Time/iter: {end_time - start_time:.2f}s, \"\n",
    "                f\"Total Loss: {metrics['total_loss']:.4f}, \"\n",
    "                f\"Value Loss: {metrics['value_loss']:.4f}, \"\n",
    "                f\"Policy Loss: {metrics['policy_loss']:.4f}\"\n",
    "            )\n",
    "    \n",
    "            # GPU stats for both GPUs\n",
    "            utilization0 = nvmlDeviceGetUtilizationRates(handle0)\n",
    "            memory0 = nvmlDeviceGetMemoryInfo(handle0)\n",
    "            utilization1 = nvmlDeviceGetUtilizationRates(handle1)\n",
    "            memory1 = nvmlDeviceGetMemoryInfo(handle1)\n",
    "    \n",
    "            # Logging into TensorBoard\n",
    "            writer.write_scalars(step, {\n",
    "                'Loss/Total': metrics['total_loss'],\n",
    "                'Loss/Value': metrics['value_loss'],\n",
    "                'Loss/Policy': metrics['policy_loss'],\n",
    "                'GPU/GPU_0_Utilization_Percent': utilization0.gpu,\n",
    "                'GPU/GPU_0_Memory_Used_MiB': memory0.used / 1024**2,\n",
    "                'GPU/GPU_1_Utilization_Percent': utilization1.gpu,\n",
    "                'GPU/GPU_1_Memory_Used_MiB': memory1.used / 1024**2\n",
    "            })\n",
    "            writer.flush()\n",
    "\n",
    "            # de-parallelizes the state so it can be serialized and saved to disk\n",
    "            state_to_save = jax.device_get( # transfers the data from device memory to CPU memory\n",
    "                jax.tree_util.tree_map(lambda x: x[0], # selects the state from device 0\n",
    "                replicated_train_state))\n",
    "            save_checkpoint(state_to_save, f'/kaggle/working/apa_jax_e2e.ckpt')\n",
    "\n",
    "    nvmlShutdown()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9192.67555,
   "end_time": "2025-09-15T10:02:19.447922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-15T07:29:06.772372",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
